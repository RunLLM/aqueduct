from typing import Optional

import pandas as pd
import pytest
from aqueduct.artifacts.base_artifact import BaseArtifact
from aqueduct.constants.enums import ArtifactType
from aqueduct.error import AqueductError, InvalidUserArgumentException
from aqueduct.integrations.mongodb_integration import MongoDBIntegration

from aqueduct import LoadUpdateMode, op
from sdk.data_integration_tests.flow_manager import FlowManager
from sdk.data_integration_tests.mongo_db_data_validator import MongoDBDataValidator
from sdk.data_integration_tests.save import save
from sdk.data_integration_tests.validation_helpers import check_hotel_reviews_table_artifact
from sdk.shared.naming import generate_table_name
from sdk.shared.validation import check_artifact_was_computed


@pytest.fixture(autouse=True)
def assert_data_integration_is_mongo_db(data_integration):
    assert isinstance(data_integration, MongoDBIntegration)


# def test_mongo_fetch(client, data_integration: MongoDBIntegration):
#     # Retrieve all rows with _id column excluded.
#     # This makes sure the `check_hotel_reviews_table_artifact` doesn't include
#     # this _id column generated by MongoDB.
#     hotel_reviews = data_integration.collection("hotel_reviews").find({}, {"_id": 0})
#     check_hotel_reviews_table_artifact(hotel_reviews)
#
#
# def test_bad_fetch(client, data_integration: MongoDBIntegration):
#     # collection that doesn't exist
#     with pytest.raises(AqueductError, match="Preview Execution Failed"):
#         data_integration.collection("missing_table").find({})
#
#     # valid collection, bad query, here "$or" is an invalid operator.
#     with pytest.raises(AqueductError, match="Preview Execution Failed"):
#         data_integration.collection("hotel_reviews").find({}, {"$or": ["1", "2"]})
#
#
# def test_mongo_fetch_column_selection(client, data_integration: MongoDBIntegration):
#     hotel_reviews = data_integration.collection("hotel_reviews").find({}, {"review": 1}).get()
#     assert list(hotel_reviews.columns) == ["_id", "review"]
#     hotel_reviews = (
#         data_integration.collection("hotel_reviews")
#         .find({}, {"_id": 0, "reviewer_nationality": 1})
#         .get()
#     )
#     assert list(hotel_reviews.columns) == ["reviewer_nationality"]
#
#
# def test_mongo_fetch_with_filter(client, data_integration: MongoDBIntegration):
#     actual_data = (
#         data_integration.collection("hotel_reviews")
#         .find({"reviewer_nationality": " United Kingdom "})
#         .get()
#     )
#     all_data = data_integration.collection("hotel_reviews").find({}).get()
#     assert len(actual_data) == len(all_data[all_data["reviewer_nationality"] == " United Kingdom "])
#
#
# def test_mongo_fetch_with_multiple_parametrized_filters(
#     client, data_integration: MongoDBIntegration
# ):
#     client.create_param("param_1", default=" United Kingdom ")
#     client.create_param("param_2", default="")
#     actual_data = (
#         data_integration.collection("hotel_reviews")
#         .find(
#             {
#                 "reviewer_nationality": {
#                     "$in": [
#                         "{{param_1}}",
#                         "{{ param_2 }}",  # ensure we can trim spaces around params properly.
#                     ]
#                 }
#             }
#         )
#         .get(parameters={"param_2": " Thailand "})
#     )
#     all_data = data_integration.collection("hotel_reviews").find({}).get()
#     assert len(actual_data) == len(
#         all_data[all_data["reviewer_nationality"].isin([" United Kingdom ", " Thailand "])]
#     )
#
#
# def test_mongo_save_replace(flow_manager: FlowManager, data_integration: MongoDBIntegration):
#     # retrieve all rows with _id column.
#     hotel_reviews = data_integration.collection("hotel_reviews").find({})
#     save(data_integration, hotel_reviews, generate_table_name(), LoadUpdateMode.REPLACE)
#
#     flow = flow_manager.publish_flow_test(hotel_reviews)
#
#     MongoDBDataValidator(flow_manager._client, data_integration).check_saved_artifact_data(
#         flow, hotel_reviews.id(), expected_data=hotel_reviews.get()
#     )
#
#
# def test_mongo_save_append(flow_manager: FlowManager, data_integration: MongoDBIntegration):
#     table_name = generate_table_name()
#
#     # saving twice with append mode
#     # Everything is done with `_id` excluded, as this field must be unique.
#     # We rely on mongoDB to generate `_id`s when we upload copies. Otherwise,
#     # append would fail if we try to upload with duplicated `_id`s .
#     hotel_reviews = data_integration.collection("hotel_reviews").find({}, {"_id": 0})
#     save(data_integration, hotel_reviews, table_name, LoadUpdateMode.REPLACE)
#     flow = flow_manager.publish_flow_test(hotel_reviews)
#     save(data_integration, hotel_reviews, table_name, LoadUpdateMode.APPEND)
#     flow = flow_manager.publish_flow_test(existing_flow=flow, artifacts=hotel_reviews)
#
#     reviews_data = hotel_reviews.get()
#     expected_data = pd.concat([reviews_data, reviews_data], ignore_index=True)
#     actual_data = data_integration.collection(table_name).find({}, {"_id": 0}).get()
#     assert expected_data.equals(actual_data)
#
#
# def test_mongo_artifact_with_custom_metadata(
#     flow_manager: FlowManager, data_integration: MongoDBIntegration
# ):
#     # TODO: validate custom descriptions once we can fetch descriptions easily.
#     op_name = "test"
#     artf_name = "test artifact"
#     description = "test description"
#     hotel_reviews = data_integration.collection("hotel_reviews").find(
#         {}, {"_id": 0}, name=op_name, description=description
#     )
#     assert hotel_reviews.name() == artf_name
#
#     flow = flow_manager.publish_flow_test(artifacts=hotel_reviews)
#     check_artifact_was_computed(flow, artf_name)
#
#
# def test_mongo_artifact_with_same_op_and_artf_names(
#     flow_manager: FlowManager, data_integration: MongoDBIntegration
# ):
#     # TODO: validate custom descriptions once we can fetch descriptions easily.
#     op_name = "test"
#     artf_name = "test"
#     description = "test description"
#     hotel_reviews = data_integration.collection("hotel_reviews").find(
#         {}, {"_id": 0}, name=op_name, output=artf_name, description=description
#     )
#     assert hotel_reviews.name() == artf_name
#
#     flow = flow_manager.publish_flow_test(artifacts=hotel_reviews)
#     check_artifact_was_computed(flow, artf_name)
#
#
# def test_mongo_preserves_bson_table_even_with_pickled_collection_type(
#     flow_manager,
#     data_integration: MongoDBIntegration,
# ):
#     """Test that bson table fidelity is preserved in the case where it is included
#     in a collection object (list, tuple).
#     """
#     hotel_reviews = data_integration.collection("hotel_reviews").find({}, {"_id": 0})
#
#     @op
#     def select_first_object_of_input_tuple(mongo_table, another_param):
#         return mongo_table
#
#     output = select_first_object_of_input_tuple(hotel_reviews, 123)
#
#     # Saving the output back to Mongo will guarantee that the table maintained fidelity
#     # across function execution.
#     table_name = generate_table_name()
#     save(data_integration, output, table_name, LoadUpdateMode.REPLACE)
#
#     flow_manager.publish_flow_test(artifacts=output)
#     saved_data = data_integration.collection(table_name).find({}, {"_id": 0}).get()
#     assert hotel_reviews.get().equals(saved_data)
