{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8db8770",
   "metadata": {},
   "source": [
    "## RoBERTa Sentiment Pipeline\n",
    "\n",
    "This notebook will create a pipeline that uses the [`twitter-roberta-base-sentiment-latest`](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) `transformers` model to analyze the sentiment of tweets about Game of Thrones Season 8. The dataset is stored in an S3 bucket for this pipeline, but you can find it [here](https://www.kaggle.com/datasets/monogenea/game-of-thrones-twitter).\n",
    "\n",
    "The code here is adapted from the example code on the HuggingFace website.\n",
    "\n",
    "In the `process` function below we limit the dataset to be the first 5,000 tweets of the datsets (which has about ~800K tweets total). Our cluster is using a `p2.xlarge` VM with a single Tesla K80, which bogs when analyzing more than a few thousand tweets. This isn't a limitation of Aqueduct's and can be solved by paying for a larger VM.\n",
    "\n",
    "Note that this notebook makes two assumptions:\n",
    "\n",
    "1. You have your Aqueduct server connected to a Kubernetes cluster with a GPU node group enabled. The easiest way to set this up is to use a hosted Kubernetes offering like AWS EKS or GKE. See our documentation for more details on connecting Aqueduct to Kubernetes.\n",
    "2. You have an object store (e.g., AWS S3) connected with the dataset from the above blog post stored in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef0b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Aqueduct client.\n",
    "import aqueduct as aq\n",
    "from aqueduct import op, metric\n",
    "\n",
    "client = aq.Client()\n",
    "\n",
    "# This config tells Aqueduct to run every operator on the resource named \"eks-us-east-2\".\n",
    "# It also activates \"lazy\" mode, meaning that we will only trigger compute operations\n",
    "# when data is requested since some of the functions below can be expensive.\n",
    "aq.global_config({\"lazy\": True, 'engine': 'eks-us-east-2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21b6a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>üëç on @YouTube: GAME OF THRONES 8x01 Breakdown!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>üëç on @YouTube: Ups and Downs From Game Of Thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Liked on YouTube: Ups and Downs From Game Of T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Liked on YouTube: GAME OF THRONES 8x01 Breakdo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@MrLegenDarius unpopular opinion: game of thro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet\n",
       "0   0  üëç on @YouTube: GAME OF THRONES 8x01 Breakdown!...\n",
       "1   1  üëç on @YouTube: Ups and Downs From Game Of Thro...\n",
       "2   2  Liked on YouTube: Ups and Downs From Game Of T...\n",
       "3   3  Liked on YouTube: GAME OF THRONES 8x01 Breakdo...\n",
       "4   4  @MrLegenDarius unpopular opinion: game of thro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the S3 bucket and see a preview of the table.\n",
    "# This is about ~100MB of data and takes about ~10s to load.\n",
    "datasets_bucket = client.resource('datasets')\n",
    "tweets = datasets_bucket.file('got_s8_tweets.csv', artifact_type=\"table\", format=\"csv\")\n",
    "\n",
    "tweets.get().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e6529",
   "metadata": {},
   "source": [
    "Here, we can see a preview of the tweets that we got from S3.\n",
    "\n",
    "Next, we're going to write an Aqueduct operator that is going to preprocess the text. We're going to clean up our tweets to account for user handles and links, and then we'll  use the [`transformers`] library's `AutoTokenizer` class to tokenize our data. \n",
    "\n",
    "We first create batches of 1K tweets to tokenize at a time. To keep our tensors uniform, we pad them with 0s as necessary and then concatenate them. \n",
    "\n",
    "The `@op` decorator here has a few configuration parameters:\n",
    "* First, the `engine` parameter tells us that we're going to be running on our EKS cluster in `us-east-2`; you can see the configuration for this resource on the Aqueduct UI. \n",
    "* Second, we specify the requirements needed to run this function (`torch` and `transformers`); if necessary, we could specify the required versions as well. \n",
    "* Finally, we tell Aqueduct to give this container 15GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd524871",
   "metadata": {},
   "outputs": [],
   "source": [
    "@op(\n",
    "    engine='eks-us-east-2', \n",
    "    requirements=['torch', 'transformers'], \n",
    "    resources={\n",
    "        'memory': '15GB',\n",
    "    },\n",
    ")\n",
    "def process(inputs, model, input_limit):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from transformers import AutoTokenizer, AutoConfig\n",
    "    from transformers.tokenization_utils_base import BatchEncoding\n",
    "    \n",
    "    # A simple helper function that replaces @-mentions and links in \n",
    "    # our tweets.\n",
    "    def split(text):\n",
    "        new_text = []\n",
    "        for t in text.split(\" \"):\n",
    "            t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "            t = 'http' if t.startswith('http') else t\n",
    "            new_text.append(t)\n",
    "        return \" \".join(new_text)\n",
    "     \n",
    "    split_text = list(map(split, inputs['tweet'][:input_limit].tolist()))\n",
    "\n",
    "    # Load the transformers configuration and tokenizer. We use `use_fast` to load\n",
    "    # the fast, Rust-based tokenizer provided by HuggingFace.\n",
    "    config = AutoConfig.from_pretrained(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "\n",
    "    batch_size = 1000\n",
    "    \n",
    "    # Create the empty tensors that we'll use to stack our tokenized data into.\n",
    "    input_ids = torch.empty(0, 0, dtype=torch.int64)\n",
    "    attention_masks = torch.empty(0, 0, dtype=torch.int64)\n",
    "    \n",
    "    # Iterate through the full dataset batch by batch, generate \n",
    "    for i in range((len(split_text) // batch_size) + 1):\n",
    "        if (i * batch_size) == len(split_text):\n",
    "            break\n",
    "            \n",
    "        if (i + 1) * batch_size > len(split_text):\n",
    "            end = len(split_text)\n",
    "        else:\n",
    "            end = (i + 1) * batch_size\n",
    "            \n",
    "        tokens = tokenizer(\n",
    "            split_text[(i * batch_size) : end], \n",
    "            max_length=500, # This is required by the model.\n",
    "            padding='max_length', \n",
    "            return_tensors='pt', \n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "            \n",
    "        # Pad the existing tensors if necessary.\n",
    "        pad_delta = np.abs(tokens['input_ids'].size(1) - input_ids.size()[1])\n",
    "        pad = (0, pad_delta)\n",
    "        if pad_delta != 0: # If the dimensions are the same, we can blindly concatenate.                           \n",
    "            if tokens['input_ids'].size()[1] > input_ids.size()[1]:\n",
    "                input_ids = torch.nn.functional.pad(input_ids, pad, \"constant\", 0)\n",
    "                attention_masks = torch.nn.functional.pad(attention_masks, pad, \"constant\", 0)\n",
    "            else:\n",
    "                tokens['input_ids'] = torch.nn.functional.pad(tokens['input_ids'], pad, \"constant\", 0)\n",
    "                tokens['attention_mask'] = torch.nn.functional.pad(tokens['attention_mask'], pad, \"constant\", 0)\n",
    "                \n",
    "        input_ids = torch.cat(\n",
    "            (\n",
    "                input_ids,\n",
    "                tokens['input_ids'],\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        attention_masks = torch.cat(\n",
    "            (\n",
    "                attention_masks,\n",
    "                tokens['attention_mask'],\n",
    "            )\n",
    "        )\n",
    "                \n",
    "    return BatchEncoding({ 'input_ids': input_ids, 'attention_mask': attention_masks })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7af339",
   "metadata": {},
   "source": [
    "We'll create an Aqueduct parameter telling us which model to use. We'll use the RoBERTa base sentiment model linked above, but if we wanted, we could swap this out in a future run.\n",
    "\n",
    "We'll invoke the `process` function on the `tweets` dataset described above. Since we're processing a large number of tweets, this function can take around ~10 minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc3cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.create_param(name=\"model\", default=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "input_limit = client.create_param(name=\"input_limit\", default=100)\n",
    "\n",
    "featurized = process(tweets, model, input_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8309bb4",
   "metadata": {},
   "source": [
    "Calling `.get()` on the `featurized` object will show us a preview of the tokenized features right here in our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9018c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator process Logs:\n",
      "stderr:\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 828kB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json: 100%|##########| 899k/899k [00:00<00:00, 10.9MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt: 100%|##########| 456k/456k [00:00<00:00, 6.58MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json: 100%|##########| 239/239 [00:00<00:00, 196kB/s]\n",
      "\n",
      "Operator process Logs:\n",
      "stderr:\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 828kB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json: 100%|##########| 899k/899k [00:00<00:00, 10.9MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt: 100%|##########| 456k/456k [00:00<00:00, 6.58MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json: 100%|##########| 239/239 [00:00<00:00, 196kB/s]\n",
      "\n",
      "Operator process Logs:\n",
      "stderr:\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 828kB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json: 100%|##########| 899k/899k [00:00<00:00, 10.9MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt: 100%|##########| 456k/456k [00:00<00:00, 6.58MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json: 100%|##########| 239/239 [00:00<00:00, 196kB/s]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 31193,  8384,  ...,     1,     1,     1],\n",
       "        [    0, 31193,  8384,  ...,     1,     1,     1],\n",
       "        [    0,   574, 21101,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,  7939,    17,  ...,     1,     1,     1],\n",
       "        [    0,  5379,  6828,  ...,     1,     1,     1],\n",
       "        [    0,  1922,    44,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurized.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eddebbb",
   "metadata": {},
   "source": [
    "Next, we'll define a function that will load the RoBERTa model and make predictions on the features defined above. Again, we batch our predictions to avoid overloading memory.\n",
    "\n",
    "Similar to the `process` function above, we use the `@op` decorator to tell Aqueudct how to run our function. The configuration here is exactly the same except for the fact that we also ask for a GPU in line 6. Based on this configuration, Aqueduct will automatically use a container with CUDA drivers installed for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24510fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@op(\n",
    "    engine='eks-us-east-2', \n",
    "    requirements=['torch', 'transformers'], \n",
    "    resources={\n",
    "        'memory': '15GB',\n",
    "        'gpu_resource_name': 'nvidia.com/gpu',\n",
    "    }\n",
    ")\n",
    "def predict(features, model):\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    from transformers import AutoConfig\n",
    "    from transformers.tokenization_utils_base import BatchEncoding\n",
    "    \n",
    "    from scipy.special import softmax\n",
    "    import numpy as np\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(model)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model).to('cuda:0')\n",
    "    \n",
    "    batch_size = 10\n",
    "    num_entries = features['input_ids'].size()[0]\n",
    "    \n",
    "    start = 0\n",
    "    pred_batches = []\n",
    "    for i in range((num_entries // batch_size) + 1):\n",
    "        if start == num_entries:\n",
    "            break\n",
    "        \n",
    "        if (i + 1) * batch_size >= num_entries:\n",
    "            end = num_entries\n",
    "        else:\n",
    "            end = (i + 1) * batch_size\n",
    "        \n",
    "        batch = BatchEncoding({\n",
    "            'input_ids': features['input_ids'][start:end],\n",
    "            'attention_mask': features['attention_mask'][start:end]\n",
    "        }).to('cuda:0')\n",
    "        batch_preds = model(**batch)\n",
    "        \n",
    "        sm = softmax(batch_preds[0].to('cpu').detach().numpy())\n",
    "        pred_batches.append(sm)\n",
    "        \n",
    "        # print(f'Processed {start} to {end}')\n",
    "        start += batch_size\n",
    "    \n",
    "    return np.concatenate(pred_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b7ffc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(featurized, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8cd36",
   "metadata": {},
   "source": [
    "When we call `.get()` on our predictions, we'll see we get three values for each tweet that we pass in, which correspond to the score for a Negative, Neutral, and Positive score, respectively. \n",
    "\n",
    "We'll postprocess our predictions to pick the maximum score for each tweet and the corresponding confidence level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fec126d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@op(\n",
    "    engine=\"eks-us-east-2\",\n",
    "    requirements=['numpy'],\n",
    "    resources={\n",
    "        'memory': '8GB'\n",
    "    },\n",
    ")\n",
    "def process_predictions(predictions):\n",
    "    import numpy as np\n",
    "    labels = [-1, 0, 1] # Use numbers here for neg./neut./pos. so we can use numpy later on.\n",
    "    \n",
    "    results = list(map(\n",
    "        lambda prediction: [labels[np.argmax(prediction)], prediction[np.argmax(prediction)]],\n",
    "        predictions,\n",
    "    ))\n",
    "    \n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d26e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = process_predictions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3cc85",
   "metadata": {},
   "source": [
    "Finally, we'll want to know the average confidence for our three classes, so we'll create thre [Aqueduct metrics](https://docs.aqueducthq.com/metrics-and-checks/metrics-measuring-your-predictions) to track the scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce8449ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator process Logs:\n",
      "stderr:\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 828kB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json: 100%|##########| 899k/899k [00:00<00:00, 10.9MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt: 100%|##########| 456k/456k [00:00<00:00, 6.58MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json: 100%|##########| 239/239 [00:00<00:00, 196kB/s]\n",
      "\n",
      "Operator process Logs:\n",
      "stderr:\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 828kB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json: 100%|##########| 899k/899k [00:00<00:00, 10.9MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt: 100%|##########| 456k/456k [00:00<00:00, 6.58MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json: 100%|##########| 239/239 [00:00<00:00, 196kB/s]\n",
      "\n",
      "Operator predict Logs:\n",
      "stderr:\n",
      "\t/opt/conda/envs/py310_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\t  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 1.19MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   2%|2         | 10.5M/501M [00:00<00:12, 40.6MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   6%|6         | 31.5M/501M [00:00<00:04, 94.4MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  13%|#2        | 62.9M/501M [00:00<00:03, 143MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  19%|#8        | 94.4M/501M [00:00<00:02, 173MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  25%|##5       | 126M/501M [00:00<00:01, 188MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  31%|###1      | 157M/501M [00:00<00:01, 199MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  38%|###7      | 189M/501M [00:01<00:01, 206MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  44%|####3     | 220M/501M [00:01<00:01, 217MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  50%|#####     | 252M/501M [00:01<00:01, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  57%|#####6    | 283M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  63%|######2   | 315M/501M [00:01<00:00, 225MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  69%|######9   | 346M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  75%|#######5  | 377M/501M [00:01<00:00, 223MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  82%|########1 | 409M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  88%|########7 | 440M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  94%|#########4| 472M/501M [00:02<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 219MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 201MB/s]\n",
      "\n",
      "Operator process Logs:\n",
      "stderr:\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 828kB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json: 100%|##########| 899k/899k [00:00<00:00, 10.9MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt: 100%|##########| 456k/456k [00:00<00:00, 6.58MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json: 100%|##########| 239/239 [00:00<00:00, 196kB/s]\n",
      "\n",
      "Operator predict Logs:\n",
      "stderr:\n",
      "\t/opt/conda/envs/py310_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\t  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 1.19MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   2%|2         | 10.5M/501M [00:00<00:12, 40.6MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   6%|6         | 31.5M/501M [00:00<00:04, 94.4MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  13%|#2        | 62.9M/501M [00:00<00:03, 143MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  19%|#8        | 94.4M/501M [00:00<00:02, 173MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  25%|##5       | 126M/501M [00:00<00:01, 188MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  31%|###1      | 157M/501M [00:00<00:01, 199MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  38%|###7      | 189M/501M [00:01<00:01, 206MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  44%|####3     | 220M/501M [00:01<00:01, 217MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  50%|#####     | 252M/501M [00:01<00:01, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  57%|#####6    | 283M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  63%|######2   | 315M/501M [00:01<00:00, 225MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  69%|######9   | 346M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  75%|#######5  | 377M/501M [00:01<00:00, 223MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  82%|########1 | 409M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  88%|########7 | 440M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  94%|#########4| 472M/501M [00:02<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 219MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 201MB/s]\n",
      "\n",
      "Operator predict Logs:\n",
      "stderr:\n",
      "\t/opt/conda/envs/py310_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\t  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 1.19MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   2%|2         | 10.5M/501M [00:00<00:12, 40.6MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   6%|6         | 31.5M/501M [00:00<00:04, 94.4MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  13%|#2        | 62.9M/501M [00:00<00:03, 143MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  19%|#8        | 94.4M/501M [00:00<00:02, 173MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  25%|##5       | 126M/501M [00:00<00:01, 188MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  31%|###1      | 157M/501M [00:00<00:01, 199MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  38%|###7      | 189M/501M [00:01<00:01, 206MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  44%|####3     | 220M/501M [00:01<00:01, 217MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  50%|#####     | 252M/501M [00:01<00:01, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  57%|#####6    | 283M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  63%|######2   | 315M/501M [00:01<00:00, 225MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  69%|######9   | 346M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  75%|#######5  | 377M/501M [00:01<00:00, 223MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  82%|########1 | 409M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  88%|########7 | 440M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  94%|#########4| 472M/501M [00:02<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 219MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 201MB/s]\n",
      "\n",
      "Operator predict Logs:\n",
      "stderr:\n",
      "\t/opt/conda/envs/py310_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\t  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 1.19MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   2%|2         | 10.5M/501M [00:00<00:12, 40.6MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   6%|6         | 31.5M/501M [00:00<00:04, 94.4MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  13%|#2        | 62.9M/501M [00:00<00:03, 143MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  19%|#8        | 94.4M/501M [00:00<00:02, 173MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  25%|##5       | 126M/501M [00:00<00:01, 188MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  31%|###1      | 157M/501M [00:00<00:01, 199MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  38%|###7      | 189M/501M [00:01<00:01, 206MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  44%|####3     | 220M/501M [00:01<00:01, 217MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  50%|#####     | 252M/501M [00:01<00:01, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  57%|#####6    | 283M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  63%|######2   | 315M/501M [00:01<00:00, 225MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  69%|######9   | 346M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  75%|#######5  | 377M/501M [00:01<00:00, 223MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  82%|########1 | 409M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  88%|########7 | 440M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  94%|#########4| 472M/501M [00:02<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 219MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 201MB/s]\n",
      "\n",
      "Average positive confidence: 0.1023180050436746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator process Logs:\n",
      "stderr:\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 828kB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json: 100%|##########| 899k/899k [00:00<00:00, 10.9MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt: 100%|##########| 456k/456k [00:00<00:00, 6.58MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json: 100%|##########| 239/239 [00:00<00:00, 196kB/s]\n",
      "\n",
      "Operator process Logs:\n",
      "stderr:\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 828kB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json: 100%|##########| 899k/899k [00:00<00:00, 10.9MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt: 100%|##########| 456k/456k [00:00<00:00, 6.58MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json: 100%|##########| 239/239 [00:00<00:00, 196kB/s]\n",
      "\n",
      "Operator predict Logs:\n",
      "stderr:\n",
      "\t/opt/conda/envs/py310_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\t  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 1.19MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   2%|2         | 10.5M/501M [00:00<00:12, 40.6MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   6%|6         | 31.5M/501M [00:00<00:04, 94.4MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  13%|#2        | 62.9M/501M [00:00<00:03, 143MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  19%|#8        | 94.4M/501M [00:00<00:02, 173MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  25%|##5       | 126M/501M [00:00<00:01, 188MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  31%|###1      | 157M/501M [00:00<00:01, 199MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  38%|###7      | 189M/501M [00:01<00:01, 206MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  44%|####3     | 220M/501M [00:01<00:01, 217MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  50%|#####     | 252M/501M [00:01<00:01, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  57%|#####6    | 283M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  63%|######2   | 315M/501M [00:01<00:00, 225MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  69%|######9   | 346M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  75%|#######5  | 377M/501M [00:01<00:00, 223MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  82%|########1 | 409M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  88%|########7 | 440M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  94%|#########4| 472M/501M [00:02<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 219MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 201MB/s]\n",
      "\n",
      "Operator process Logs:\n",
      "stderr:\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 828kB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json: 100%|##########| 899k/899k [00:00<00:00, 10.9MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt: 100%|##########| 456k/456k [00:00<00:00, 6.58MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json: 100%|##########| 239/239 [00:00<00:00, 196kB/s]\n",
      "\n",
      "Operator predict Logs:\n",
      "stderr:\n",
      "\t/opt/conda/envs/py310_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\t  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 1.19MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   2%|2         | 10.5M/501M [00:00<00:12, 40.6MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   6%|6         | 31.5M/501M [00:00<00:04, 94.4MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  13%|#2        | 62.9M/501M [00:00<00:03, 143MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  19%|#8        | 94.4M/501M [00:00<00:02, 173MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  25%|##5       | 126M/501M [00:00<00:01, 188MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  31%|###1      | 157M/501M [00:00<00:01, 199MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  38%|###7      | 189M/501M [00:01<00:01, 206MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  44%|####3     | 220M/501M [00:01<00:01, 217MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  50%|#####     | 252M/501M [00:01<00:01, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  57%|#####6    | 283M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  63%|######2   | 315M/501M [00:01<00:00, 225MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  69%|######9   | 346M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  75%|#######5  | 377M/501M [00:01<00:00, 223MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  82%|########1 | 409M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  88%|########7 | 440M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  94%|#########4| 472M/501M [00:02<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 219MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 201MB/s]\n",
      "\n",
      "Operator predict Logs:\n",
      "stderr:\n",
      "\t/opt/conda/envs/py310_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\t  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 1.19MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   2%|2         | 10.5M/501M [00:00<00:12, 40.6MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   6%|6         | 31.5M/501M [00:00<00:04, 94.4MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  13%|#2        | 62.9M/501M [00:00<00:03, 143MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  19%|#8        | 94.4M/501M [00:00<00:02, 173MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  25%|##5       | 126M/501M [00:00<00:01, 188MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  31%|###1      | 157M/501M [00:00<00:01, 199MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  38%|###7      | 189M/501M [00:01<00:01, 206MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  44%|####3     | 220M/501M [00:01<00:01, 217MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  50%|#####     | 252M/501M [00:01<00:01, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  57%|#####6    | 283M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  63%|######2   | 315M/501M [00:01<00:00, 225MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  69%|######9   | 346M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  75%|#######5  | 377M/501M [00:01<00:00, 223MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  82%|########1 | 409M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  88%|########7 | 440M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  94%|#########4| 472M/501M [00:02<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 219MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 201MB/s]\n",
      "\n",
      "Operator predict Logs:\n",
      "stderr:\n",
      "\t/opt/conda/envs/py310_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\t  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 1.19MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   2%|2         | 10.5M/501M [00:00<00:12, 40.6MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   6%|6         | 31.5M/501M [00:00<00:04, 94.4MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  13%|#2        | 62.9M/501M [00:00<00:03, 143MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  19%|#8        | 94.4M/501M [00:00<00:02, 173MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  25%|##5       | 126M/501M [00:00<00:01, 188MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  31%|###1      | 157M/501M [00:00<00:01, 199MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  38%|###7      | 189M/501M [00:01<00:01, 206MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  44%|####3     | 220M/501M [00:01<00:01, 217MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  50%|#####     | 252M/501M [00:01<00:01, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  57%|#####6    | 283M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  63%|######2   | 315M/501M [00:01<00:00, 225MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  69%|######9   | 346M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  75%|#######5  | 377M/501M [00:01<00:00, 223MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  82%|########1 | 409M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  88%|########7 | 440M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  94%|#########4| 472M/501M [00:02<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 219MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 201MB/s]\n",
      "\n",
      "Average negative confidence: 0.07094896460572879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator process Logs:\n",
      "stderr:\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 828kB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json: 100%|##########| 899k/899k [00:00<00:00, 10.9MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt: 100%|##########| 456k/456k [00:00<00:00, 6.58MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json: 100%|##########| 239/239 [00:00<00:00, 196kB/s]\n",
      "\n",
      "Operator process Logs:\n",
      "stderr:\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 828kB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json: 100%|##########| 899k/899k [00:00<00:00, 10.9MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt: 100%|##########| 456k/456k [00:00<00:00, 6.58MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json: 100%|##########| 239/239 [00:00<00:00, 196kB/s]\n",
      "\n",
      "Operator predict Logs:\n",
      "stderr:\n",
      "\t/opt/conda/envs/py310_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\t  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 1.19MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   2%|2         | 10.5M/501M [00:00<00:12, 40.6MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   6%|6         | 31.5M/501M [00:00<00:04, 94.4MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  13%|#2        | 62.9M/501M [00:00<00:03, 143MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  19%|#8        | 94.4M/501M [00:00<00:02, 173MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  25%|##5       | 126M/501M [00:00<00:01, 188MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  31%|###1      | 157M/501M [00:00<00:01, 199MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  38%|###7      | 189M/501M [00:01<00:01, 206MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  44%|####3     | 220M/501M [00:01<00:01, 217MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  50%|#####     | 252M/501M [00:01<00:01, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  57%|#####6    | 283M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  63%|######2   | 315M/501M [00:01<00:00, 225MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  69%|######9   | 346M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  75%|#######5  | 377M/501M [00:01<00:00, 223MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  82%|########1 | 409M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  88%|########7 | 440M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  94%|#########4| 472M/501M [00:02<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 219MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 201MB/s]\n",
      "\n",
      "Operator process Logs:\n",
      "stderr:\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 828kB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/vocab.json: 100%|##########| 899k/899k [00:00<00:00, 10.9MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)olve/main/merges.txt: 100%|##########| 456k/456k [00:00<00:00, 6.58MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)cial_tokens_map.json: 100%|##########| 239/239 [00:00<00:00, 196kB/s]\n",
      "\n",
      "Operator predict Logs:\n",
      "stderr:\n",
      "\t/opt/conda/envs/py310_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\t  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 1.19MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   2%|2         | 10.5M/501M [00:00<00:12, 40.6MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   6%|6         | 31.5M/501M [00:00<00:04, 94.4MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  13%|#2        | 62.9M/501M [00:00<00:03, 143MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  19%|#8        | 94.4M/501M [00:00<00:02, 173MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  25%|##5       | 126M/501M [00:00<00:01, 188MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  31%|###1      | 157M/501M [00:00<00:01, 199MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  38%|###7      | 189M/501M [00:01<00:01, 206MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  44%|####3     | 220M/501M [00:01<00:01, 217MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  50%|#####     | 252M/501M [00:01<00:01, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  57%|#####6    | 283M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  63%|######2   | 315M/501M [00:01<00:00, 225MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  69%|######9   | 346M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  75%|#######5  | 377M/501M [00:01<00:00, 223MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  82%|########1 | 409M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  88%|########7 | 440M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  94%|#########4| 472M/501M [00:02<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 219MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 201MB/s]\n",
      "\n",
      "Operator predict Logs:\n",
      "stderr:\n",
      "\t/opt/conda/envs/py310_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\t  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 1.19MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   2%|2         | 10.5M/501M [00:00<00:12, 40.6MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   6%|6         | 31.5M/501M [00:00<00:04, 94.4MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  13%|#2        | 62.9M/501M [00:00<00:03, 143MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  19%|#8        | 94.4M/501M [00:00<00:02, 173MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  25%|##5       | 126M/501M [00:00<00:01, 188MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  31%|###1      | 157M/501M [00:00<00:01, 199MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  38%|###7      | 189M/501M [00:01<00:01, 206MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  44%|####3     | 220M/501M [00:01<00:01, 217MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  50%|#####     | 252M/501M [00:01<00:01, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  57%|#####6    | 283M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  63%|######2   | 315M/501M [00:01<00:00, 225MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  69%|######9   | 346M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  75%|#######5  | 377M/501M [00:01<00:00, 223MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  82%|########1 | 409M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  88%|########7 | 440M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  94%|#########4| 472M/501M [00:02<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 219MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 201MB/s]\n",
      "\n",
      "Operator predict Logs:\n",
      "stderr:\n",
      "\t/opt/conda/envs/py310_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\t  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\t\r",
      "\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)lve/main/config.json: 100%|##########| 929/929 [00:00<00:00, 1.19MB/s]\n",
      "\t\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   2%|2         | 10.5M/501M [00:00<00:12, 40.6MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:   6%|6         | 31.5M/501M [00:00<00:04, 94.4MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  13%|#2        | 62.9M/501M [00:00<00:03, 143MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  19%|#8        | 94.4M/501M [00:00<00:02, 173MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  25%|##5       | 126M/501M [00:00<00:01, 188MB/s] \r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  31%|###1      | 157M/501M [00:00<00:01, 199MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  38%|###7      | 189M/501M [00:01<00:01, 206MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  44%|####3     | 220M/501M [00:01<00:01, 217MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  50%|#####     | 252M/501M [00:01<00:01, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  57%|#####6    | 283M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  63%|######2   | 315M/501M [00:01<00:00, 225MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  69%|######9   | 346M/501M [00:01<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  75%|#######5  | 377M/501M [00:01<00:00, 223MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  82%|########1 | 409M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  88%|########7 | 440M/501M [00:02<00:00, 221MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";:  94%|#########4| 472M/501M [00:02<00:00, 222MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 219MB/s]\r",
      "\tDownloading (‚Ä¶)\"pytorch_model.bin\";: 100%|##########| 501M/501M [00:02<00:00, 201MB/s]\n",
      "\n",
      "Average neutral confidence: 0.07419255116220677\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "@metric(requirements=['numpy'])\n",
    "def avg_positive(labels):\n",
    "    return np.mean(list(map (\n",
    "        lambda label: label[1],\n",
    "        filter(\n",
    "            lambda label: label[0] == 1,\n",
    "            labels\n",
    "        )\n",
    "    )))\n",
    "\n",
    "@metric(requirements=['numpy'])\n",
    "def avg_negative(labels):\n",
    "    return np.mean(list(map (\n",
    "        lambda label: label[1],\n",
    "        filter(\n",
    "            lambda label: label[0] == -1,\n",
    "            labels\n",
    "        )\n",
    "    )))\n",
    "\n",
    "@metric(requirements=['numpy'])\n",
    "def avg_neutral(labels):\n",
    "    return np.mean(list(map (\n",
    "        lambda label: label[1],\n",
    "        filter(\n",
    "            lambda label: label[0] == 0,\n",
    "            labels\n",
    "        )\n",
    "    )))\n",
    "\n",
    "avg_pos = avg_positive(labels)\n",
    "avg_neg = avg_negative(labels)\n",
    "avg_neut = avg_neutral(labels)\n",
    "\n",
    "print(f'Average positive confidence: {avg_pos.get()}')\n",
    "print(f'Average negative confidence: {avg_neg.get()}')\n",
    "print(f'Average neutral confidence: {avg_neut.get()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e49ea2",
   "metadata": {},
   "source": [
    "And that's it! We're finished creating our workflow and are ready to publish it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "client.publish_flow(\n",
    "    'RoBERTa Tweet Sentiment',\n",
    "    dedent('''\n",
    "    Uses the HuggingFace RoBERTa Tweet sentiment model to analyze the\n",
    "    sentiment of tweets about Game of Thrones season 8. \n",
    "    '''),\n",
    "    artifacts=[labels]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
